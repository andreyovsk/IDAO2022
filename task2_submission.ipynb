{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 2 Submission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Redefined ```prepare_dataset()``` in ```baseline.py```."
      ],
      "metadata": {
        "id": "ExCikCUVRI_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset_path, test_mode):\n",
        "    if test_mode == 'private':\n",
        "        dataset_path = Path(dataset_path)\n",
        "        #targets = pd.read_csv(dataset_path / \"targets.csv\", index_col=0)\n",
        "        struct = {\n",
        "            item.name.strip(\".json\"): read_pymatgen_dict(item)\n",
        "            for item in (dataset_path / \"structures\").iterdir()\n",
        "        }\n",
        "\n",
        "        data = pd.DataFrame(columns=[\"structures\"], index=struct.keys())\n",
        "        data = data.assign(structures=struct.values(), targets=0)\n",
        "\n",
        "        return data\n",
        "\n",
        "    elif test_mode == 'public':\n",
        "        dataset_path = Path(dataset_path)\n",
        "        targets = pd.read_csv(dataset_path / \"targets.csv\", index_col=0)\n",
        "        struct = {\n",
        "            item.name.strip(\".json\"): read_pymatgen_dict(item)\n",
        "            for item in (dataset_path / \"structures\").iterdir()\n",
        "        }\n",
        "\n",
        "        data = pd.DataFrame(columns=[\"structures\"], index=struct.keys())\n",
        "        data = data.assign(structures=struct.values(), targets=targets)\n",
        "\n",
        "        return train_test_split(data, test_size=0.25, random_state=666) # creepy\n",
        "    \n",
        "    else:\n",
        "        print('Invalid test_mode specified')"
      ],
      "metadata": {
        "id": "AzlklAS0RKGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redefined ```main()``` function in ```generate_submission.py```."
      ],
      "metadata": {
        "id": "FIflDRZZPwHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESV-uF79PkQl"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from baseline import prepare_model\n",
        "from baseline import read_pymatgen_dict\n",
        "from baseline import prepare_dataset\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "\n",
        "\n",
        "def main(config):\n",
        "    public = prepare_dataset(config['datapath'], test_mode = 'public')\n",
        "    private = prepare_dataset(config['test_datapath'], test_mode = 'private')\n",
        "    df_train = public[0]\n",
        "    df_test = public[1]    # validation\n",
        "    pr_test = private # test\n",
        "    \n",
        "    # number of atoms in the lattice\n",
        "    def atoms_num(i):\n",
        "        return len(i.as_dict()['sites'])\n",
        "    \n",
        "    for k in [df_train, df_test, pr_test]: \n",
        "        k['atoms_num'] = k.iloc[:, 0].map(atoms_num)\n",
        "        \n",
        "        \n",
        "    def atoms_per_lattice(i):\n",
        "        sites = i.as_dict()['sites']\n",
        "        return set([sites[j]['label'] for j in range(len(sites))])\n",
        "    \n",
        "    sets_atoms_per_lattice = df_train.iloc[:, 0].map(atoms_per_lattice)\n",
        "    \n",
        "    unique_atoms_train = set() \n",
        "    for i in sets_atoms_per_lattice:\n",
        "        unique_atoms_train = unique_atoms_train.union(i)\n",
        "    \n",
        "    # only four atoms!!!\n",
        "    unique_atoms_train = list(unique_atoms_train)\n",
        "    \n",
        "    from collections import Counter\n",
        "    \n",
        "    # Preprocessing Train   \n",
        "    count_atoms_per_lattice = dict()\n",
        "    for i in unique_atoms_train:\n",
        "        count_atoms_per_lattice[i] = []\n",
        "        \n",
        "        \n",
        "    for i in df_train.iloc[:, 0]:\n",
        "        sites = i.as_dict()['sites']\n",
        "        count_atoms = Counter([sites[j]['label'] for j in range(len(sites))])\n",
        "        for j in unique_atoms_train:\n",
        "            if j in count_atoms.keys():\n",
        "                count_atoms_per_lattice[j].append(count_atoms[j])\n",
        "            else:\n",
        "                count_atoms_per_lattice[j].append(0)\n",
        "            \n",
        "    df_train['Mo_count'] = count_atoms_per_lattice['Mo'] \n",
        "    df_train['S_count'] = count_atoms_per_lattice['S']\n",
        "    df_train['Se_count'] = count_atoms_per_lattice['Se']\n",
        "    df_train['W_count'] = count_atoms_per_lattice['W']\n",
        "    \n",
        "    # Preprocessing Test\n",
        "    count_atoms_per_lattice_test = dict()\n",
        "    for i in unique_atoms_train: \n",
        "        count_atoms_per_lattice_test[i] = []\n",
        "    \n",
        "    for i in df_test.iloc[:, 0]:\n",
        "        sites = i.as_dict()['sites']\n",
        "        count_atoms = Counter([sites[j]['label'] for j in range(len(sites))])\n",
        "        for j in unique_atoms_train:  \n",
        "            if j in count_atoms.keys():\n",
        "                count_atoms_per_lattice_test[j].append(count_atoms[j])\n",
        "            else:\n",
        "                count_atoms_per_lattice_test[j].append(0)\n",
        "    \n",
        "    df_test['Mo_count'] = count_atoms_per_lattice_test['Mo'] \n",
        "    df_test['S_count'] = count_atoms_per_lattice_test['S']\n",
        "    df_test['Se_count'] = count_atoms_per_lattice_test['Se']\n",
        "    df_test['W_count'] = count_atoms_per_lattice_test['W']\n",
        "    \n",
        "    ##### Preprocessing Private\n",
        "    count_atoms_per_lattice_pr_test = dict()\n",
        "    for i in unique_atoms_train:\n",
        "        count_atoms_per_lattice_pr_test[i] = []\n",
        "    \n",
        "    for i in pr_test.iloc[:, 0]:\n",
        "        sites = i.as_dict()['sites']\n",
        "        count_atoms = Counter([sites[j]['label'] for j in range(len(sites))])\n",
        "        for j in unique_atoms_train:\n",
        "            if j in count_atoms.keys():\n",
        "                count_atoms_per_lattice_pr_test[j].append(count_atoms[j])\n",
        "            else:\n",
        "                count_atoms_per_lattice_pr_test[j].append(0)\n",
        "    \n",
        "    pr_test['Mo_count'] = count_atoms_per_lattice_pr_test['Mo']\n",
        "    pr_test['S_count'] = count_atoms_per_lattice_pr_test['S']\n",
        "    pr_test['Se_count'] = count_atoms_per_lattice_pr_test['Se']\n",
        "    pr_test['W_count'] = count_atoms_per_lattice_pr_test['W']\n",
        "    \n",
        "    \n",
        "    ## choosing only needed columns and creating binary variables\n",
        "    df_train1 = df_train[['Mo_count', 'W_count', 'Se_count', 'S_count', 'targets']]\n",
        "    df_train1 = pd.get_dummies(df_train1, drop_first = True, columns = ['Mo_count', 'W_count', 'Se_count', 'S_count'])\n",
        "    \n",
        "    df_test1 = df_test[['Mo_count', 'W_count', 'Se_count', 'S_count', 'targets']]\n",
        "    df_test1 = pd.get_dummies(df_test1, drop_first = True, columns = ['Mo_count', 'W_count', 'Se_count', 'S_count'])   \n",
        "    \n",
        "    # same for Private\n",
        "    pr_test1 = pr_test[['Mo_count', 'W_count', 'Se_count', 'S_count', 'targets']]\n",
        "    pr_test1 = pd.get_dummies(pr_test1, drop_first = True, columns = ['Mo_count', 'W_count', 'Se_count', 'S_count'])\n",
        "    \n",
        "    for i in df_train1.columns:\n",
        "        if i not in df_test1.columns:\n",
        "            df_test1[i] = 0\n",
        "            # adding\n",
        "        if i not in pr_test1.columns:\n",
        "            pr_test1[i] = 0\n",
        "    \n",
        "    \n",
        "    df_test1.columns = df_train1.columns\n",
        "    # adding\n",
        "    pr_test1.columns = df_train1.columns\n",
        "    \n",
        "    df_train1['atoms_count_group'] = 0\n",
        "    for i, name in enumerate(df_train1.columns[1:7]):\n",
        "        df_train1['atoms_count_group'] = df_train1['atoms_count_group'] + df_train1[name] * 10**(6-i)\n",
        "    \n",
        "    df_test1['atoms_count_group'] = 0\n",
        "    for i, name in enumerate(df_test1.columns[1:7]):\n",
        "        df_test1['atoms_count_group'] = df_test1['atoms_count_group'] + df_test1[name] * 10**(6-i)\n",
        "    \n",
        "    # for private\n",
        "    \n",
        "    pr_test1['atoms_count_group'] = 0\n",
        "    for i, name in enumerate(pr_test1.columns[1:7]):\n",
        "        pr_test1['atoms_count_group'] = pr_test1['atoms_count_group'] + pr_test1[name] * 10**(6-i)\n",
        "        \n",
        "        \n",
        "        \n",
        "    \n",
        "    ################################ adding 10000 group creator\n",
        "    # selecting only 10000 group\n",
        "    df_train1 = df_train1.reset_index()\n",
        "    df_test1 = df_test1.reset_index()\n",
        "    pr_test1 = pr_test1.reset_index()\n",
        "    \n",
        "    df_train1_10000 = df_train1[df_train1['atoms_count_group'] == 10000].reset_index().copy()\n",
        "    train_index_10000 = df_train1[df_train1['atoms_count_group'] == 10000].index\n",
        "    \n",
        "    df_test1_10000 = df_test1[df_test1['atoms_count_group'] == 10000].reset_index().copy()\n",
        "    test_index_10000 = df_test1[df_test1['atoms_count_group'] == 10000].index\n",
        "    \n",
        "    df_private1_10000 = pr_test1[pr_test1['atoms_count_group'] == 10000].reset_index().copy()\n",
        "    private_index_10000 = pr_test1[pr_test1['atoms_count_group'] == 10000].index\n",
        "    # abc of Se atom\n",
        "    Se_abc_10000_train = []\n",
        "    for i in train_index_10000.to_list():                         \n",
        "        sample = df_train.iloc[i, 0].as_dict()['sites']\n",
        "        for j in sample:\n",
        "            if j['label'] == 'Se':\n",
        "                Se_abc_10000_train.append(j['abc'])\n",
        "    \n",
        "    \n",
        "    # do the same for the test\n",
        "    Se_abc_10000_test = []\n",
        "    for i in test_index_10000.to_list():                          \n",
        "        sample = df_test.iloc[i, 0].as_dict()['sites']\n",
        "        for j in sample:\n",
        "            if j['label'] == 'Se':\n",
        "                Se_abc_10000_test.append(j['abc'])\n",
        "    \n",
        "    # do the same for the private\n",
        "    Se_abc_10000_private = []\n",
        "    for i in private_index_10000.to_list():                       \n",
        "        sample = private.iloc[i, 0].as_dict()['sites']\n",
        "        for j in sample:\n",
        "            if j['label'] == 'Se':\n",
        "                Se_abc_10000_private.append(j['abc'])\n",
        "                \n",
        "    # creating dataframe from that \n",
        "    df_Se_abc_10000_train = pd.DataFrame(Se_abc_10000_train)\n",
        "    df_Se_abc_10000_train.columns = ['a', 'b', 'c']\n",
        "    \n",
        "    df_Se_abc_10000_test = pd.DataFrame(Se_abc_10000_test)\n",
        "    df_Se_abc_10000_test.columns = ['a', 'b', 'c']\n",
        "    \n",
        "    df_Se_abc_10000_private = pd.DataFrame(Se_abc_10000_private)\n",
        "    df_Se_abc_10000_private.columns = ['a', 'b', 'c']\n",
        "    # adding to current df\n",
        "    df_train1_10000['Se_a'] = round(df_Se_abc_10000_train['a'], 3)\n",
        "    df_train1_10000['Se_b'] = round(df_Se_abc_10000_train['b'], 3)\n",
        "    df_train1_10000['Se_c'] = round(df_Se_abc_10000_train['c'], 3)\n",
        "    \n",
        "    df_test1_10000['Se_a'] = round(df_Se_abc_10000_test['a'], 3)\n",
        "    df_test1_10000['Se_b'] = round(df_Se_abc_10000_test['b'], 3)\n",
        "    df_test1_10000['Se_c'] = round(df_Se_abc_10000_test['c'], 3)\n",
        "    \n",
        "    df_private1_10000['Se_a'] = round(df_Se_abc_10000_private['a'], 3)\n",
        "    df_private1_10000['Se_b'] = round(df_Se_abc_10000_private['b'], 3)\n",
        "    df_private1_10000['Se_c'] = round(df_Se_abc_10000_private['c'], 3)\n",
        "    # creating all 64 possible coordinates for Molibden and each layer of S (ANY GROUP IS THE SAME!)\n",
        "    # Molibden\n",
        "    Mo_a_set = []\n",
        "    Mo_b_set = []\n",
        "    Mo_c_set = []\n",
        "    # S, 1st and 2nd layer\n",
        "    S_a_set = []\n",
        "    S_b_set = []\n",
        "    S_c_set = []\n",
        "    \n",
        "    for val, i in enumerate(df_train.iloc[0, 0].as_dict()['sites']):\n",
        "        if val < 63:\n",
        "            Mo_a_set.append(round(i['abc'][0], 3))\n",
        "            Mo_b_set.append(round(i['abc'][1], 3))\n",
        "            Mo_c_set.append(round(i['abc'][2], 3))\n",
        "        # skipping Se atom\n",
        "        elif val > 64:\n",
        "            S_a_set.append(round(i['abc'][0], 3))\n",
        "            S_b_set.append(round(i['abc'][1], 3))\n",
        "            S_c_set.append(round(i['abc'][2], 3))\n",
        "    \n",
        "    #print('Molibden')\n",
        "    Mo_a_set = sorted(list(set(Mo_a_set)))\n",
        "    Mo_b_set = sorted(list(set(Mo_b_set)))\n",
        "    Mo_c_set = sorted(list(set(Mo_c_set)))[0]\n",
        "    #print(Mo_a_set)\n",
        "    #print(Mo_b_set)\n",
        "    #print(Mo_c_set)\n",
        "    \n",
        "    #print('S')\n",
        "    S_a_set = sorted(list(set(S_a_set)))\n",
        "    S_b_set = sorted(list(set(S_b_set)))\n",
        "    S_c_set = sorted(list(set(S_c_set)))\n",
        "    #print(S_a_set)\n",
        "    #print(S_b_set)\n",
        "    #print(S_c_set)\n",
        "    # it's just vice versa a and b for Mo and S!!!\n",
        "    # creating list of 64 places for Mo and 128 places for S in the correct order\n",
        "    Mo_64_coordinates = []\n",
        "    for i in Mo_a_set:\n",
        "        for j in Mo_b_set:\n",
        "            Mo_64_coordinates.append([i, j, Mo_c_set])\n",
        "    \n",
        "    S_128_coordinates = []\n",
        "    for i in S_c_set:\n",
        "        for j in S_a_set:\n",
        "            for k in S_b_set:\n",
        "                S_128_coordinates.append([j, k, i])\n",
        "    # DETECTING MISSED ATOMS(USING ALGORITHM ABOVE) FOR ALL LATTICES IN THE GROUP 10000\n",
        "    num_molibden_atoms = 63\n",
        "    num_rubbish_atoms = 1\n",
        "    num_S_atoms = 126\n",
        "    \n",
        "    Mo_missed = 64 - num_molibden_atoms\n",
        "    S_missed = 128 - num_S_atoms\n",
        "    \n",
        "    missed_atoms_coordinates_group10000_train = []\n",
        "    missed_atoms_coordinates_group10000_test = []\n",
        "    missed_atoms_coordinates_group10000_private = []\n",
        "    \n",
        "    indices = [train_index_10000, test_index_10000, private_index_10000]\n",
        "    coordinates_list = [missed_atoms_coordinates_group10000_train,\n",
        "                        missed_atoms_coordinates_group10000_test,\n",
        "                        missed_atoms_coordinates_group10000_private]\n",
        "    datasets = [df_train, df_test, private]\n",
        "    \n",
        "    for ind in range(3):\n",
        "        for j in indices[ind]:\n",
        "            missed_atoms_coordinates_lattice = []\n",
        "    \n",
        "            Mo_atom_missed_counter = 0\n",
        "            S_atom_missed_counter = 0\n",
        "    \n",
        "            lattice_sample = datasets[ind].iloc[j, 0].as_dict()['sites']\n",
        "    \n",
        "            for val, i in enumerate(lattice_sample):\n",
        "                # first Mo atom\n",
        "                if val == 0 and round(i['abc'][1], 3) != 0.083:\n",
        "                    missed_atoms_coordinates_lattice = missed_atoms_coordinates_lattice + Mo_64_coordinates[val]\n",
        "                    Mo_atom_missed_counter += 1\n",
        "                # other Mo atoms\n",
        "                elif val > 0 and val < num_molibden_atoms:\n",
        "                    diff_b_with_prev = round(i['abc'][1] - lattice_sample[val - 1]['abc'][1], 3)\n",
        "                    if (diff_b_with_prev != 0.125) & (diff_b_with_prev != -0.875):\n",
        "                        missed_atoms_coordinates_lattice = missed_atoms_coordinates_lattice + Mo_64_coordinates[val]\n",
        "                        Mo_atom_missed_counter += 1\n",
        "    \n",
        "                # first S atom\n",
        "                elif val == num_molibden_atoms + num_rubbish_atoms:\n",
        "                    if round(i['abc'][1], 3) != 0.042:\n",
        "                        missed_atoms_coordinates_lattice = missed_atoms_coordinates_lattice + S_128_coordinates[val - num_molibden_atoms - num_rubbish_atoms + S_atom_missed_counter]\n",
        "                        S_atom_missed_counter += 1\n",
        "    \n",
        "                # other S atoms\n",
        "                elif val > num_molibden_atoms + num_rubbish_atoms:\n",
        "                    diff_b_with_prev = round(i['abc'][1] - lattice_sample[val - 1]['abc'][1], 3)\n",
        "                    if (diff_b_with_prev != 0.125) & (diff_b_with_prev != -0.875):\n",
        "                        missed_atoms_coordinates_lattice = missed_atoms_coordinates_lattice + S_128_coordinates[val - num_molibden_atoms - num_rubbish_atoms + S_atom_missed_counter]\n",
        "                        S_atom_missed_counter += 1\n",
        "                        # Case if missed atoms are together. This is special case to be algorithmed below.\n",
        "                        if (diff_b_with_prev != 0.25) & (diff_b_with_prev != -0.75):\n",
        "                            missed_atoms_coordinates_lattice = missed_atoms_coordinates_lattice + S_128_coordinates[val - num_molibden_atoms - num_rubbish_atoms + S_atom_missed_counter]\n",
        "                            S_atom_missed_counter += 1\n",
        "                            break\n",
        "    \n",
        "            # if missed atoms are on last positions, so the algorithm above does not catch them\n",
        "            # Molibden\n",
        "            if Mo_atom_missed_counter != Mo_missed:\n",
        "                missed_atoms_coordinates_lattice = Mo_64_coordinates[63] + missed_atoms_coordinates_lattice\n",
        "                Mo_atom_missed_counter += 1\n",
        "                \n",
        "            # S\n",
        "            if S_atom_missed_counter - S_missed == -2:\n",
        "                missed_atoms_coordinates_lattice = missed_atoms_coordinates_lattice + S_128_coordinates[126]\n",
        "                missed_atoms_coordinates_lattice = missed_atoms_coordinates_lattice + S_128_coordinates[127]\n",
        "                S_atom_missed_counter += 2\n",
        "            elif S_atom_missed_counter - S_missed == -1:\n",
        "                missed_atoms_coordinates_lattice = missed_atoms_coordinates_lattice + S_128_coordinates[127]\n",
        "                S_atom_missed_counter += 1\n",
        "    \n",
        "            coordinates_list[ind].append(missed_atoms_coordinates_lattice)\n",
        "    # df of missed atoms\n",
        "    df_missed_coord_10000_train = pd.DataFrame(missed_atoms_coordinates_group10000_train)\n",
        "    df_missed_coord_10000_test = pd.DataFrame(missed_atoms_coordinates_group10000_test)\n",
        "    df_missed_coord_10000_private = pd.DataFrame(missed_atoms_coordinates_group10000_private)\n",
        "    main_datasets = [df_train1_10000, df_test1_10000, df_private1_10000]\n",
        "    prelim_datasets = [df_missed_coord_10000_train, df_missed_coord_10000_test, df_missed_coord_10000_private]\n",
        "    \n",
        "    for i in range(3):\n",
        "        main_datasets[i]['missed_Mo_a'] = prelim_datasets[i][0]\n",
        "        main_datasets[i]['missed_Mo_b'] = prelim_datasets[i][1]\n",
        "        main_datasets[i]['missed_Mo_c'] = prelim_datasets[i][2]\n",
        "        main_datasets[i]['missed_S1_a'] = prelim_datasets[i][3]\n",
        "        main_datasets[i]['missed_S1_b'] = prelim_datasets[i][4]\n",
        "        main_datasets[i]['missed_S1_c'] = prelim_datasets[i][5]\n",
        "        main_datasets[i]['missed_S2_a'] = prelim_datasets[i][6]\n",
        "        main_datasets[i]['missed_S2_b'] = prelim_datasets[i][7]\n",
        "        main_datasets[i]['missed_S2_c'] = prelim_datasets[i][8]\n",
        "    cols_to_leave = ['level_0', 'index', 'targets', 'Se_a', 'Se_b', 'Se_c',\n",
        "          'missed_Mo_a', 'missed_Mo_b', 'missed_Mo_c', 'missed_S1_a',\n",
        "          'missed_S1_b', 'missed_S1_c', 'missed_S2_a', 'missed_S2_b',\n",
        "          'missed_S2_c']\n",
        "    \n",
        "    train10000 = df_train1_10000[cols_to_leave]\n",
        "    test10000 = df_test1_10000[cols_to_leave]\n",
        "    private10000 = df_private1_10000[cols_to_leave]\n",
        "    def substantial_impurity_detector(i):\n",
        "        if round(i['Se_a'], 3) == i['missed_S1_a'] and round(i['Se_b'], 3) == i['missed_S1_b'] and round(i['Se_c'], 3) == i['missed_S1_c']:\n",
        "            return 1\n",
        "        elif round(i['Se_a'], 3) == i['missed_S2_a'] and round(i['Se_b'], 3) == i['missed_S2_b'] and round(i['Se_c'], 3) == i['missed_S2_c']:\n",
        "            return 2\n",
        "        else:\n",
        "            return 0\n",
        "    train10000['impurity_atom'] = train10000.apply(substantial_impurity_detector, axis = 1)\n",
        "    test10000['impurity_atom'] = test10000.apply(substantial_impurity_detector, axis = 1)\n",
        "    private10000['impurity_atom'] = private10000.apply(substantial_impurity_detector, axis = 1)\n",
        "    train10000_new = train10000.copy()\n",
        "    test10000_new = test10000.copy()\n",
        "    private10000_new = private10000.copy()\n",
        "    \n",
        "    new_dfs = []\n",
        "    \n",
        "    for df in [train10000_new, test10000_new, private10000_new]:\n",
        "        df['vacancyS_a'] = df.apply(lambda x: x['missed_S1_a'] if x['impurity_atom'] == 2 else x['missed_S2_a'], axis = 1)\n",
        "        df['vacancyS_b'] = df.apply(lambda x: x['missed_S1_b'] if x['impurity_atom'] == 2 else x['missed_S2_b'], axis = 1)\n",
        "        df['vacancyS_c'] = df.apply(lambda x: x['missed_S1_c'] if x['impurity_atom'] == 2 else x['missed_S2_c'], axis = 1)\n",
        "    \n",
        "        df['Se_a'] = np.round(df['Se_a'], 3)\n",
        "        df['Se_b'] = np.round(df['Se_b'], 3)\n",
        "        df['Se_c'] = np.round(df['Se_c'], 3)\n",
        "    \n",
        "        cols_to_leave = ['level_0', 'index', 'targets', 'Se_a', 'Se_b', 'Se_c',\n",
        "              'missed_Mo_a', 'missed_Mo_b', 'missed_Mo_c', 'vacancyS_a', 'vacancyS_b', 'vacancyS_c']\n",
        "    \n",
        "        df = df[cols_to_leave].copy()\n",
        "    \n",
        "        new_col_names = ['level_0', 'index', 'targets', 'Se_impurityS_a', 'Se_impurityS_b', 'Se_impurityS_c',\n",
        "              'vacancyMo_a', 'vacancyMo_b', 'vacancyMo_c', 'vacancyS_a', 'vacancyS_b', 'vacancyS_c']\n",
        "        df.columns = new_col_names\n",
        "    \n",
        "        new_dfs.append(df)\n",
        "    \n",
        "    train10000_new = new_dfs[0]\n",
        "    test10000_new = new_dfs[1]\n",
        "    private10000_new = new_dfs[2]\n",
        "    # FROM OBSERVATION OF THE COORDINATES\n",
        "    mean_a = 0.5\n",
        "    mean_b = 0.5\n",
        "    mean_c = 0.25\n",
        "    \n",
        "    for df in [train10000_new, test10000_new, private10000_new]:\n",
        "        df['Se_impurityS_dist_to_center'] = np.sqrt((df['Se_impurityS_a']-mean_a)**2 + (df['Se_impurityS_b']-mean_b)**2 + (df['Se_impurityS_c']-mean_c)**2)\n",
        "        df['vacancyMo_dist_to_center'] = np.sqrt((df['vacancyMo_a']-mean_a)**2 + (df['vacancyMo_b']-mean_b)**2 + (df['vacancyMo_c']-mean_c)**2)\n",
        "        df['vacancyS_dist_to_center'] = np.sqrt((df['vacancyS_a']-mean_a)**2 + (df['vacancyS_b']-mean_b)**2 + (df['vacancyS_c']-mean_c)**2)\n",
        "    train_df10000 = train10000_new.copy()\n",
        "    valid_df10000 = test10000_new.copy()\n",
        "    private_df10000 = private10000_new.copy()\n",
        "    # another 3 features, All pairwise distances between the vacancy and impurity coordinates\n",
        "    dfs = [train_df10000, valid_df10000, private_df10000]\n",
        "    for df in dfs:\n",
        "        df['dist_Se_impurityS_vacancyMo'] = np.sqrt((df['Se_impurityS_a'] - df['vacancyMo_a'])**2\\\n",
        "                                                            + (df['Se_impurityS_b'] - df['vacancyMo_b'])**2\\\n",
        "                                                            + (df['Se_impurityS_c'] - df['vacancyMo_c'])**2)\n",
        "        df['dist_Se_impurityS_vacancyS'] = np.sqrt((df['Se_impurityS_a'] - df['vacancyS_a'])**2\\\n",
        "                                                                + (df['Se_impurityS_b'] - df['vacancyS_b'])**2\\\n",
        "                                                                + (df['Se_impurityS_c'] - df['vacancyS_c'])**2)\n",
        "        df['dist_vacancyMo_vacancyS'] = np.sqrt((df['vacancyMo_a'] - df['vacancyS_a'])**2\\\n",
        "                                                            + (df['vacancyMo_b'] - df['vacancyS_b'])**2\\\n",
        "                                                            + (df['vacancyMo_c'] - df['vacancyS_c'])**2)\n",
        "    \n",
        "    # euclidean sum of distances\n",
        "    for df in dfs:\n",
        "        df['eucl_sum_distance'] = np.sqrt(df['dist_Se_impurityS_vacancyMo']**2 +\\\n",
        "                                          df['dist_Se_impurityS_vacancyS']**2 +\\\n",
        "                                          df['dist_vacancyMo_vacancyS']**2)\n",
        "        \n",
        "    # another 3 features: distances from the origin\n",
        "    for df in dfs:\n",
        "        df['dist_Se_impurityS_origin'] = np.sqrt((df['Se_impurityS_a'])**2\\\n",
        "                                                              + (df['Se_impurityS_b'])**2\\\n",
        "                                                              + (df['Se_impurityS_c'])**2)\n",
        "        df['dist_vacancyS_origin'] = np.sqrt((df['vacancyS_a'])**2\\\n",
        "                                                          + (df['vacancyS_b'])**2\\\n",
        "                                                          + (df['vacancyS_c'])**2)\n",
        "        df['dist_vacancyMo_origin'] = np.sqrt((df['vacancyMo_a'])**2\\\n",
        "                                                          + (df['vacancyMo_b'])**2\\\n",
        "                                                          + (df['vacancyMo_c'])**2)\n",
        "    def cos_creator1(i):\n",
        "        dist2D = np.sqrt(i['dist_vacancyMo_vacancyS']**2 - (i['vacancyMo_c'] - i['vacancyS_c'])**2)\n",
        "        if dist2D == 0:\n",
        "            return 0\n",
        "        # pivot is vacancyMo:\n",
        "        elif i['vacancyMo_c'] < i['vacancyS_c']:\n",
        "            return (i['vacancyS_a'] - i['vacancyMo_a']) / dist2D\n",
        "        # pivot is vacancyS\n",
        "        else:\n",
        "            return (i['vacancyMo_a'] - i['vacancyS_a']) / dist2D\n",
        "    \n",
        "    def sin_creator1(i):\n",
        "        dist2D = np.sqrt(i['dist_vacancyMo_vacancyS']**2 - (i['vacancyMo_c'] - i['vacancyS_c'])**2)\n",
        "        if dist2D == 0:\n",
        "            return 0\n",
        "        # pivot is vacancyMo:\n",
        "        elif i['vacancyMo_c'] < i['vacancyS_c']:\n",
        "            return (i['vacancyS_b'] - i['vacancyMo_b']) / dist2D\n",
        "        else:\n",
        "            return (i['vacancyMo_b'] - i['vacancyS_b']) / dist2D\n",
        "    \n",
        "    \n",
        "    def cos_creator2(i):\n",
        "        dist2D = np.sqrt(i['dist_Se_impurityS_vacancyS']**2 - (i['Se_impurityS_c'] - i['vacancyS_c'])**2)\n",
        "        if dist2D == 0:\n",
        "            return 0\n",
        "        # pivot is Se_impurityS:\n",
        "        elif i['Se_impurityS_c'] < i['vacancyS_c']:\n",
        "            return (i['vacancyS_a'] - i['Se_impurityS_a']) / dist2D\n",
        "        # pivot is vacancyS\n",
        "        else:\n",
        "            return (i['Se_impurityS_a'] - i['vacancyS_a']) / dist2D\n",
        "    \n",
        "    def sin_creator2(i):\n",
        "        dist2D = np.sqrt(i['dist_Se_impurityS_vacancyS']**2 - (i['Se_impurityS_c'] - i['vacancyS_c'])**2)\n",
        "        if dist2D == 0:\n",
        "            return 0\n",
        "        # pivot is Se_impurityS:\n",
        "        elif i['Se_impurityS_c'] < i['vacancyS_c']:\n",
        "            return (i['vacancyS_b'] - i['Se_impurityS_b']) / dist2D\n",
        "        else:\n",
        "            return (i['Se_impurityS_b'] - i['vacancyS_b']) / dist2D\n",
        "    \n",
        "    \n",
        "    def cos_creator3(i):\n",
        "        dist2D = np.sqrt(i['dist_Se_impurityS_vacancyMo']**2 - (i['vacancyMo_c'] - i['Se_impurityS_c'])**2)\n",
        "        if dist2D == 0:\n",
        "            return 0\n",
        "        # pivot is vacancyMo:\n",
        "        elif i['vacancyMo_c'] < i['Se_impurityS_c']:\n",
        "            return (i['Se_impurityS_a'] - i['vacancyMo_a']) / dist2D\n",
        "        # pivot is Se_impurityS\n",
        "        else:\n",
        "            return (i['vacancyMo_a'] - i['Se_impurityS_a']) / dist2D\n",
        "    \n",
        "    def sin_creator3(i):\n",
        "        dist2D = np.sqrt(i['dist_Se_impurityS_vacancyMo']**2 - (i['vacancyMo_c'] - i['Se_impurityS_c'])**2)\n",
        "        if dist2D == 0:\n",
        "            return 0\n",
        "        # pivot is vacancyMo:\n",
        "        elif i['vacancyMo_c'] < i['Se_impurityS_c']:\n",
        "            return (i['Se_impurityS_b'] - i['vacancyMo_b']) / dist2D\n",
        "        else:\n",
        "            return (i['vacancyMo_b'] - i['Se_impurityS_b']) / dist2D\n",
        "    # applying tricky angle\n",
        "    dfs = [train_df10000, valid_df10000, private_df10000]\n",
        "    for df in dfs:\n",
        "        df['sin_tricky1'] = df.apply(sin_creator1, axis = 1)\n",
        "        df['cos_tricky1'] = df.apply(cos_creator1, axis = 1)\n",
        "        df['tan_tricky1'] = df['sin_tricky1'] / (df['cos_tricky1'] + 0.00001)\n",
        "        df['ctg_tricky1'] = df['cos_tricky1'] / (df['sin_tricky1'] + 0.00001)\n",
        "    \n",
        "        df['sin_tricky2'] = df.apply(sin_creator2, axis = 1)\n",
        "        df['cos_tricky2'] = df.apply(cos_creator2, axis = 1)\n",
        "        df['tan_tricky2'] = df['sin_tricky2'] / (df['cos_tricky2'] + 0.00001)\n",
        "        df['ctg_tricky2'] = df['cos_tricky2'] / (df['sin_tricky2'] + 0.00001)\n",
        "    \n",
        "        df['sin_tricky3'] = df.apply(sin_creator3, axis = 1)\n",
        "        df['cos_tricky3'] = df.apply(cos_creator3, axis = 1)\n",
        "        df['tan_tricky3'] = df['sin_tricky3'] / (df['cos_tricky3'] + 0.00001)\n",
        "        df['ctg_tricky3'] = df['cos_tricky3'] / (df['sin_tricky3'] + 0.00001)\n",
        "    # another 2 features, binary variables, depending on the C coordinates of the atoms on S places\n",
        "    dfs = [train_df10000, valid_df10000, private_df10000]\n",
        "    for df in dfs:\n",
        "        df['C_atomsS_same'] = (df['vacancyS_c'] == df['Se_impurityS_c']).astype('int')\n",
        "        df['vacancyS_c_0_355'] = (df['vacancyS_c'] == 0.355).astype('int')\n",
        "    \n",
        "   \n",
        "    \n",
        "    ################################# ending 10000 group creator: train_df10000, valid_df10000, private_df10000 as an output\n",
        "    \n",
        "    \n",
        "    # prep feautres and run AutoML\n",
        "    train_df10000.drop(['vacancyMo_c'],axis=1, inplace=True)\n",
        "    valid_df10000.drop(['vacancyMo_c'],axis=1, inplace=True)\n",
        "    private_df10000.drop(['vacancyMo_c'],axis=1, inplace=True)\n",
        "  \n",
        "    X_train = train_df10000.drop(['targets', 'level_0', 'index'], axis=1)\n",
        "    y_train = train_df10000.targets\n",
        "    X_test = valid_df10000.drop(['targets', 'level_0', 'index'], axis=1)\n",
        "    y_test = valid_df10000.targets\n",
        "    X_priv = private_df10000.drop(['targets', 'level_0', 'index'], axis=1)\n",
        "    # no targets for priv\n",
        "    y_zero = private_df10000.targets\n",
        "    \n",
        "    \n",
        "    \n",
        "    gb = GradientBoostingRegressor(loss='huber', criterion='squared_error', n_estimators = 1000, max_depth = 1, learning_rate = 1)\n",
        "    gb.fit(X_train, y_train)\n",
        "    priv_pred_gr10000 = gb.predict(X_priv)\n",
        "    df_priv_pred_gr10000 = pd.DataFrame(data={'predictions': priv_pred_gr10000}, index=private_df10000['index'])\n",
        "    \n",
        "    \n",
        "        \n",
        "    df_train1 = df_train1.set_index('index')\n",
        "    df_test1 = df_test1.set_index('index')\n",
        "    pr_test1 = pr_test1.set_index('index') \n",
        "    \n",
        "    #y_pred_train_easy = df_train1.groupby('atoms_count_group')['targets'].transform('median')\n",
        "    #y_pred_test_easy = df_test1.groupby('atoms_count_group')['targets'].transform('median')\n",
        "    df_pub = pd.concat([df_train1, df_test1], axis=0)\n",
        "    \n",
        "    # add-in: replace median with mode\n",
        "    # 0.02 LENGTH WINDOW\n",
        "    for d in [df_pub, df_train1, df_test1]:\n",
        "        # MAKING UNIVERSAL ROUND, SO THERE'LL BE ROUND NOT ONLY TO CLOSEST 0.01, 0.1, 0.001 ETC, BUT FOR 0.005, 0.0025, ETC\n",
        "        d['targets_round'] = np.round(d['targets'] / 0.02) * 0.02\n",
        "    \n",
        "    max_mode_0_02_dic = df_pub.groupby('atoms_count_group')['targets_round'].agg(pd.Series.mode).to_dict()\n",
        "    \n",
        "    for key, val in max_mode_0_02_dic.items():\n",
        "        max_mode_0_02_dic[key] = np.max(max_mode_0_02_dic[key])\n",
        "    \n",
        "    output = pr_test1['atoms_count_group'].map(max_mode_0_02_dic)\n",
        "    \n",
        "    \n",
        "    output = output.rename('predictions')\n",
        "    df_output = pd.DataFrame(output)\n",
        "    df_output.update(df_priv_pred_gr10000)\n",
        "    df_output.to_csv('./submission.csv', index_label='id')\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(\"config.yaml\") as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    main(config)"
      ]
    }
  ]
}